{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjFPzJfAalRDbSkSRXCYrU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywen2021/CPSC390/blob/main/LORA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDMMU-p5TiDF"
      },
      "outputs": [],
      "source": [
        "#https://anirbansen2709.medium.com/finetuning-llms-using-lora-77fb02cbbc48\n",
        "#!pip install git+https://github.com/huggingface/accelerate.git\n",
        "#!pip install git+https://github.com/huggingface/transformers.git\n",
        "#!pip install bitsandbytes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datasets"
      ],
      "metadata": {
        "id": "myv8zSUFhVVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "IPvZjankgom8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mentioning datatypes for better documentation\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset, load_dataset, disable_caching\n",
        "disable_caching() ## disable huggingface cache\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "ks9uvA2dTprK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Preparation\n",
        "dataset = load_dataset(\"MBZUAI/LaMini-instruction\" , split = 'train')\n",
        "small_dataset = dataset.select([i for i in range(200)])\n",
        "print(small_dataset)\n",
        "print(small_dataset[0])\n",
        "\n",
        "# creating templates\n",
        "prompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: {instruction}\\n Response:\"\"\"\n",
        "answer_template = \"\"\"{response}\"\"\"\n",
        "\n",
        "# creating function to add keys in the dictionary for prompt, answer and whole text\n",
        "def _add_text(rec):\n",
        "    instruction = rec[\"instruction\"]\n",
        "    response = rec[\"response\"]\n",
        "    # check if both exists, else raise error\n",
        "    if not instruction:\n",
        "        raise ValueError(f\"Expected an instruction in: {rec}\")\n",
        "    if not response:\n",
        "        raise ValueError(f\"Expected a response in: {rec}\")\n",
        "    rec[\"prompt\"] = prompt_template.format(instruction=instruction)\n",
        "    rec[\"answer\"] = answer_template.format(response=response)\n",
        "    rec[\"text\"] = rec[\"prompt\"] + rec[\"answer\"]\n",
        "    return rec\n",
        "\n",
        "# running through all samples\n",
        "small_dataset = small_dataset.map(_add_text)\n",
        "print(small_dataset[0])"
      ],
      "metadata": {
        "id": "mceWv7l_WSfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the tokenizer for dolly model. The tokenizer converts raw text into tokens\n",
        "model_id = \"databricks/dolly-v2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#loading the model using AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # use_cache=False,\n",
        "    device_map=\"auto\", #\"balanced\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# resizes input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "VvyG0sMzWXe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OmwJT7d4XIvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import copy\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "# Function to generate token embeddings from text part of batch\n",
        "def _preprocess_batch(batch: Dict[str, List]):\n",
        "    model_inputs = tokenizer(batch[\"text\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')\n",
        "    model_inputs[\"labels\"] = copy.deepcopy(model_inputs['input_ids'])\n",
        "    return model_inputs\n",
        "\n",
        "_preprocessing_function = partial(_preprocess_batch)\n",
        "\n",
        "# apply the preprocessing function to each batch in the dataset\n",
        "encoded_small_dataset = small_dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"instruction\", \"response\", \"prompt\", \"answer\"],\n",
        ")\n",
        "processed_dataset = encoded_small_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\n",
        "\n",
        "# splitting dataset\n",
        "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
        "print(split_dataset)\n",
        "\n",
        "# takes a list of samples from a Dataset and collate them into a batch, as a dictionary of PyTorch tensors.\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "        model = model, tokenizer=tokenizer, max_length=MAX_LENGTH, pad_to_multiple_of=8, padding='max_length')"
      ],
      "metadata": {
        "id": "qN_IMMC_WaA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "LORA_R = 256 # 512\n",
        "LORA_ALPHA = 512 # 1024\n",
        "LORA_DROPOUT = 0.05\n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        "                 r = LORA_R, # the dimension of the low-rank matrices\n",
        "                 lora_alpha = LORA_ALPHA, # scaling factor for the weight matrices\n",
        "                 lora_dropout = LORA_DROPOUT, # dropout probability of the LoRA layers\n",
        "                 bias=\"none\",\n",
        "                 task_type=\"CAUSAL_LM\",\n",
        "                 target_modules=[\"query_key_value\"],\n",
        ")\n",
        "\n",
        "# Prepare int-8 model for training - utility function that prepares a PyTorch model for int8 quantization training. <https://huggingface.co/docs/peft/task_guides/int8-asr>\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "# initialize the model with the LoRA framework\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "M_UZsqHqWced"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import bitsandbytes\n",
        "# define the training arguments first.\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
        "training_args = TrainingArguments(\n",
        "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
        "                    overwrite_output_dir=True,\n",
        "                    fp16=True, #converts to float precision 16 using bitsandbytes\n",
        "                    per_device_train_batch_size=1,\n",
        "                    per_device_eval_batch_size=1,\n",
        "                    learning_rate=LEARNING_RATE,\n",
        "                    num_train_epochs=EPOCHS,\n",
        "                    logging_strategy=\"epoch\",\n",
        "                    eval_strategy=\"epoch\",\n",
        "                    save_strategy=\"epoch\",\n",
        ")\n",
        "# training the model\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "        train_dataset=split_dataset['train'],\n",
        "        eval_dataset=split_dataset[\"test\"],\n",
        "        data_collator=data_collator,\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "# only saves the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.\n",
        "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
        "# save the full model and the training arguments\n",
        "trainer.save_model(MODEL_SAVE_FOLDER_NAME)\n",
        "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
      ],
      "metadata": {
        "id": "fiP4gUIVXM97"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}